---
title: "Statistical Analysis, MSCA 31007, Lecture 4"
author: "Hope Foster-Reyes"
date: "November 6, 2016"
output: pdf_document
geometry: margin=0.75in
---

# Analysis of Residuals of a Linear Model

Understand analysis of residuals of the estimated linear model.

_Notes_

* _Style Guide:_ https://google.github.io/styleguide/Rguide.xml

* _Packages required: note_

* _Files required: ResidualAnalysisProjectData_1.csv; store in RStudio project directory_

```{r settings, echo=FALSE}
options(scipen = 5)
```

## 1 Data

Import and plot the sample data.

```{r import}
# Import linear model ('lm') data
lm.data <- read.csv("ResidualAnalysisProjectData_1.csv")
```

Look at the sample in the file LinearModelCase1.csv. The first rows and the X-Y plot are:

```{r q1}
head(lm.data)
```

Plot the data.

```{r q1-plot}
plot(lm.data$Input, lm.data$Output, ylim = c(-5, 5))
```

## 2 Fitting the Linear Model

Estimate a linear model using function `lm()`. Examine the output of the function.

```{r q2}
lm.estimate <- lm(Output ~ Input, data = lm.data)
names(lm.estimate)
```

### 2.1 Object `lm()`

Explore the elements of the object `lm`:

#### 1. Coefficients

The coefficients are our estimate of the slope and intercept of the estimated linear model. We can output our results from our `lm.estimate` object. If we plot these as slope and intercept (while maintaining the same axis), we can compare our plot to the plot above of the data provided:

```{r q2.1-coeff}
lm.estimate$coefficients

lm.est.b <- lm.estimate$coefficients[1]
lm.est.a <- lm.estimate$coefficients[2]

lm.est.y <- lm.est.a * lm.data$Input + lm.est.b
plot(lm.data$Input, lm.est.y, ylim = c(-5, 5))
```

#### 2. Residuals (make a plot). How are the residuals calculated?

We calculate the residuals by subtracting our predicted linear Y values from the actual Y values in our sample, giving us the "error" or randomness in Y. We will refer to these residuals as Epsilon or `$\varepsilon$ as per the following equation:

$Y = aX + b + \varepsilon$

```{r q2.1-residuals}
lm.est.eps <- lm.data$Output - lm.est.y
plot(lm.est.eps)
```

#### 3. Investigate: What are `fitted.values` in our `lm()` output?

We might suspect from the name that "fitted values" refers to some sort of estimate on Y. Considering that the goal of `lm()` is to fit our data in a linear model, could it be that the fitted values are the pure values of Y from the linear relationship, without residuals? Let's test this.

```{r q2.1-fitted}
head(lm.estimate$fitted.values)
head(lm.est.y)

(all.equal(as.vector(lm.estimate$fitted.values), lm.est.y))
```

We have confirmed that the `lm()` output of `fitted.values` represents the sum of the intercept coefficient and the input (or predictor) variable multiplied by the slope coefficient, in other words our resultant output or response variable of our linear function based on our estimated coefficients, without residuals.

### 2. The `summary()` Output

Look at the summary.

```{r q2.2}
(lm.est.summary <- summary(lm.estimate))
```

Let's interpret the summary.

The `Call` section simply reminds us of our call to the function, which provides the data and specifies which variable is our independent (predictor or input variable) and which is our dependent (response or output variable). Note that we denote this by first listing our dependent variable, a tilde symbol, followed by our independent variable.

This is followed bye the `Residuals` section which provides the five number summary of the residuals. What may be interesting to us here is that the summary is nearly symmetrical, and the quartiles seem to divide the data nearly evenly, as opposed to the 1Q and 3Q values being closer to the Median as you might expect in a normal distribution. A quick boxplot confirms that the shape, while not uniform, appears to be more uniform than a normal distribution:

```{r q2.2-boxplot, fig.width=5.2, fig.height=3.6}
boxplot(lm.est.eps)
hist(lm.est.eps)
```

Next, the `Coefficients` section lists our estimated linear coefficients. The first will always be our Intercept (which, essentially, is a coefficient to the number 1), followed by the coefficients of our remaining predictor variables. In this case there is only one predictor variable, which we labeled Input. Our Input coefficient, is positive, leading to a positive correlation between Input (X) and Output (Y), which is also demonstrated by our plots above.

It is worth noting that [R documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sigma.html) states that the term "residual standard error" that we see in our `summary` output is a misnomer, and the proper term here is residual standard deviation.

Our summary output also includes this calculation of the residual standard deviation, or $\sigma$ as described below, as well as R-squared (also notated as $r^{2}$ or $\rho^{2}$) which is our "coefficient of determination" or "squared correlation coefficient". Whereas the correlation coefficient, "r" measures the strength and the direction of a linear relationship between two variables, the square of the correlation coefficient, "r squared", which we see in our summary, measures the proportion of our data that is explained by the linear relationship. In this case roughly 83% of the variation in our Output is explained by our linear model.

??? XXX ???

What is `summary(lm.est)$sigma`?

```{r q2.2-sigma}
names(lm.est.summary)
lm.est.summary$sigma
lm.est.summary$sigma^2
```

In this case "sigma" refers to the standard deviation of our residuals, also known as the residual standard deviation. It describes the amount that the residuals vary or spread from their mean, which is theoretically zero, and hence the amount that our output variable varies or spreads from a pure linear relationship with our input variable(s). For a simple Gaussian residual, this would be the $\sigma$ value in the below equation:

$Y = aX + b + \varepsilon$

$\varepsilon \sim Norm(0, \sigma)$

Check how `sigma` is calculated in the summary object by reproducing the square of it:

1. Using `var()` (the resulting variable is sigmaSquared.byVar)

```{r q2.2-manual-1}
lm.sigma.sq.byvar <- var(lm.est.eps)
```

2. Using only `sum()` (the resulting variable is sigmaSquared.bySum)

```{r q2.2-manual-2}
#XXX ???
lm.sigma.sq.bysum <- sum(lm.est.eps - mean(lm.est.eps))^2
```

Compare the two calculations with the `summary` output:

```{r q2.2-confirm}
c(lm.sigma.sq.byvar, lm.sigma.sq.bysum, lm.est.summary$sigma^2)
```

## 3 Analysis of Residuals

### 3.1 Residuals of the model

Observe the residuals, plot them against the input. Also plot their probability density in comparison with the normal density.

```{r 3.1}
lm.est.eps <- lm.estimate$residuals
plot(lm.data$Input, lm.est.eps)

lm.est.eps.density <- density(lm.est.eps)
plot(lm.est.eps.density, ylim = c(0, 0.5))
lines(lm.est.eps.density$x, 
      dnorm(lm.est.eps.density$x, mean = mean(lm.est.eps), sd = sd(lm.est.eps)))
```

***What do you conclude from the analysis of residuals?***

An observation of the density of the residuals is multi-modal and seems to indicate that we may be looking at two different normal distributions.

### 3.2 Clustering the Sample

Calculate the mean values of negative residuals and positive residuals.

```{r q3.2-1}
c(Left.Mean = mean(lm.est.eps[lm.est.eps < 0]), 
  Right.Mean = mean(lm.est.eps[lm.est.eps > 0]))
```

Separate the given sample into 2 subsamples:

* One in which the residuals are below zero

* One in which they are above zero

Create a selection variable which estimates switching between the two subsamples (a value of 1 in this variable corresponds to the positive residual case, and a value of 0 corresponds to the negative residual case).

```{r q3.2-2}
select.seq.unscrambled <- as.integer(lm.est.eps > 0)
head(select.seq.unscrambled, 30)

ChoosePos <- function(j) {
  vapply(seq_along(select.seq.unscrambled), 
         function(i) {switch(select.seq.unscrambled[i] + 1, NA, lm.data[i, j])},
         numeric(1))
}

ChooseNeg <- function(j) {
  vapply(seq_along(select.seq.unscrambled), 
         function(i) {switch(select.seq.unscrambled[i] + 1, lm.data[i, j], NA)},
         numeric(1))
}

lm.subset1 <- sapply(seq_along(lm.data), ChoosePos)
lm.subset2 <- sapply(seq_along(lm.data), ChooseNeg)

head(cbind(lm.subset1, lm.subset2), 30)

# Plot the two clusters
matplot(lm.data$Input, cbind(lm.subset1[, 2], lm.subset2[,2]), 
        type = "p", col = c("green", "blue"), pch = 19, ylab = "Separated Subsamples")

plot(select.seq.unscrambled[1:100], type = "s")
```

We've divided our data into two separate sets based on a guess (an "assumption") that: 

* Rather than looking at a *single* population whose linear correlation between predictor and response behaves according to certain linear coefficients and a Gaussian residual with certain parameters,

* We are instead looking at *two* populations each of whose linear correlation between predictor and response behaves according to *a different set* of linear coefficients and residual different parameters.

In other words, we have moved to analyzing our data set according to the following two equations:

Subsample 1 (green dots): $Y_{1} = a_{1}X_{1} + b_{1} + \varepsilon_{1}$

Subsample 1 (blue dots): $Y_{2} = a_{2}X_{2} + b_{2} + \varepsilon_{2}$

### 3.3 Confusion Matrix

The problem we have, of course, is that even if we are correct that we are indeed observing two different populations, we cannot know which of our data sets belongs to which population.

Our sequence above is the map of our "guesses". We created `select.seq.unscrambled` and assigned it the following values:

* 1 = Subsample 1

* 0 = Subsample 2

Now, this is a simulation, so let's say that hypothetically we had access to the training data set and we knew in fact far every data point whether it was in Subsample 1 or the Subsample 2. 

There is a common device used to compare our estimated sequence, `select.seq.unscrambled` and the true selection sequence, let's say this was called `select.seq.true` and was mapped as above. This device is called the confusion matrix. 

To create a confusion matrix in this case, we would call the function `confusion.Matrix` from the library `caret` as follows:

`cm <- confusionMatrix(select.seq.unscrambled, select.seq.true)$table`

Its output would look something like the following:

Group | Actual True | Actual False
-|-------------|--------------
Predicted True | 400 | 90
Predicted False | 20 | 410

Let's hard code these values:

```{r q3.3}
cm <- matrix(c(400, 90, 20, 410), ncol = 2, byrow = T, 
             dimnames = list(c("Predicted.T", "Predicted.F"),
                             c("Actual.T", "Actual.F")))
cm
```

We can derive the following from the elements `cm(Predicted, Actual)` of the resultant matrix:

* Number of True Negatives: `cm(0,0)`
* Number of True Positives: `cm(1,1)`
* Number of False Negatives: `cm(0,1)`
* Number of False Positives: `cm(1,0)`

## 4 Estimating models for subsamples

### 4.1 Fitting models

Now estimate the linear models from the subsamples.

```{r q4.1}
lm.subset1.estimate <- lm(Output ~ Input, 
                          data = data.frame(Input = lm.subset1[,1], Output = lm.subset1[,2]))
lm.subset2.estimate <- lm(Output ~ Input, 
                          data = data.frame(Input = lm.subset2[,1], Output = lm.subset2[,2]))
lm.subset1.est.summary <- summary(lm.subset1.estimate)
lm.subset2.est.summary <- summary(lm.subset2.estimate)

lm.subset1.est.summary$coefficients
lm.subset1.est.summary$sigma
lm.subset1.est.summary$df
lm.subset1.est.summary$r.squared
lm.subset1.est.summary$adj.r.squared

lm.subset2.est.summary$coefficients
lm.subset2.est.summary$sigma
lm.subset2.est.summary$df
lm.subset2.est.summary$r.squared
lm.subset2.est.summary$adj.r.squared
```

And write a summary comparing the fit of the subsamples to the whole sample:

* For the sigma parameters:

```{r q4.1-sigma}
c(lm.est.summary$sigma,
  lm.subset1.est.summary$sigma,
  lm.subset2.est.summary$sigma)
```

* For the $\rho^2$:

```{r q4.1-rho.sq}
c(lm.est.summary$r.squared,
  lm.subset1.est.summary$r.squared,
  lm.subset2.est.summary$r.squared)
```

* For the F-statistics:

```{r q4.1-f}
rbind(LinearModel=lm.est.summary$fstatistic,
      LinearModelPositive=lm.subset1.est.summary$fstatistic,
      LinearModelNegative=lm.subset2.est.summary$fstatistic)
```

Here is how we can calculate p-values of F-test using cumulative probability function of F-distribution:

```{r q4.1-f-manual}
c(LinearModel=pf(lm.est.summary$fstatistic[1], 
                 lm.est.summary$fstatistic[2], 
                 lm.est.summary$fstatistic[3],lower.tail = FALSE),
  LinearModelPositive=pf(lm.subset1.est.summary$fstatistic[1], 
                         lm.subset1.est.summary$fstatistic[2], 
                         lm.subset1.est.summary$fstatistic[3],lower.tail = FALSE),
  LinearModelNegative=pf(lm.subset2.est.summary$fstatistic[1], 
                         lm.subset2.est.summary$fstatistic[2], 
                         lm.subset2.est.summary$fstatistic[3],lower.tail = FALSE))
```

The numbers may not look exactly the same as in `summary()` because of the precision limitation.

Compare the combined residuals of the two separated models with the residuals of our original estimated model.

```{r q4.1-plot}
# Plot residuals
matplot(cbind(MixedModelResiduals=c(lm.subset1.est.summary$residuals,
                                    lm.subset2.est.summary$residuals),
              SingleModelResiduals=lm.est.summary$residuals),
        type="p", pch=16, ylab="Residuals before and after unscrambling")

# Estimate standard deviations
apply(cbind(MixedModelResiduals=c(lm.subset1.est.summary$residuals,
                                   lm.subset2.est.summary$residuals),
            SingleModelResiduals=lm.est.summary$residuals),2,sd)
```

***What is the difference between the quality of fit?***

??? XXX ???

***What is the difference between the two estimated models?***

??? XXX ???

***Try to guess how the model data were simulated and with what parameters?***

??? XXX ???