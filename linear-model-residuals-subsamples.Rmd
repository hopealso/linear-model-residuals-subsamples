---
title: "Statistical Analysis, MSCA 31007, Lecture 4"
author: "Hope Foster-Reyes"
date: "November 6, 2016"
output: pdf_document
geometry: margin=0.75in
---

# Analysis of Residuals of a Linear Model

Understand analysis of residuals of the estimated linear model.

_Notes_

* _Style Guide:_ https://google.github.io/styleguide/Rguide.xml

* _Packages required: note_

* _Files required: ResidualAnalysisProjectData_1.csv; store in RStudio project directory_

```{r settings, echo=FALSE}
options(scipen = 5)
```

## 1 Data

Import and plot the sample data.

```{r import}
# Import linear model ('lm') data
sample.data <- read.csv("ResidualAnalysisProjectData_1.csv")
```

Look at the sample in the file LinearModelCase1.csv. The first rows and the X-Y plot are:

```{r q1}
head(sample.data)
```

Plot the data.

```{r q1-plot}
plot(sample.data$Input, sample.data$Output, ylim = c(-5, 5))
```

## 2 Fitting the Linear Model

Estimate a linear model using function `lm()`. Examine the output of the function.

```{r q2}
lm.estimate <- lm(Output ~ Input, data = sample.data)
names(lm.estimate)
```

### 2.1 Object `lm()`

Explore the elements of the object `lm`:

#### 1. Coefficients

The coefficients are our estimate of the slope and intercept of the estimated linear model. We can output our results from our `lm.estimate` object. If we plot these as slope and intercept (while maintaining the same axis), we can compare our plot to the plot above of the data provided:

```{r q2.1-coeff}
lm.estimate$coefficients

lm.est.b <- lm.estimate$coefficients[1]
lm.est.a <- lm.estimate$coefficients[2]

lm.est.y <- lm.est.a * sample.data$Input + lm.est.b
plot(sample.data$Input, lm.est.y, ylim = c(-5, 5))
```

#### 2. Residuals (make a plot). How are the residuals calculated?

We calculate the residuals by subtracting our predicted linear Y values from the actual Y values in our sample, giving us the "error" or randomness in Y. We will refer to these residuals as Epsilon or `$\varepsilon$ as per the following equation:

$Y = aX + b + \varepsilon$

```{r q2.1-residuals}
lm.est.eps <- sample.data$Output - lm.est.y
# The manual calculation and lm() output are nearly identical
(mean(lm.estimate$residuals - lm.est.eps))
(sum(lm.estimate$residuals - lm.est.eps))

plot(lm.estimate$residuals)
```

#### 3. Investigate: What are `fitted.values` in our `lm()` output?

We might suspect, from the name, that "fitted values" refers to some sort of estimate on Y. Considering that the goal of `lm()` is to fit our data to a linear model, could it be that the fitted values are the pure values of Y from the linear relationship, without residuals? Let's test this.

```{r q2.1-fitted}
head(lm.estimate$fitted.values)
head(lm.est.y)

(all.equal(as.vector(lm.estimate$fitted.values), lm.est.y))
```

We have confirmed that the `lm()` output of `fitted.values` represents the sum of the intercept coefficient and the input (or predictor) variable multiplied by the slope coefficient, in other words our resultant output or response variable of our linear function based on our estimated coefficients, without residuals.

### 2. The `summary()` Output

Look at the summary.

```{r q2.2}
(lm.est.summary <- summary(lm.estimate))
```

***Let's interpret the summary.***

#### Call

The `Call` section simply reminds us of our call to the function, which provides the data and specifies which variable is our independent (predictor or input variable) and which is our dependent (response or output variable). Note that we denote this by first listing our dependent variable, followed by a tilde symbol, and finally our independent variable.

#### Residuals

This is followed by the `Residuals` section which provides the five number summary of the residuals. What may be interesting to us here is that the summary is nearly symmetrical, and the quartiles seem to divide the data nearly evenly, as opposed to the 1Q and 3Q values being much closer to the Median as you might expect in a normal distribution. A quick boxplot confirms that the shape, while not uniform, appears to be more uniform than a normal distribution:

```{r q2.2-boxplot, fig.width=5.2, fig.height=3.6}
par(mfrow=c(1,3))
boxplot(lm.est.eps, main="Sample Epsilon")
boxplot(rnorm(5000), main="Standard Normal Dist.")
boxplot(runif(500, -2, 2), main="Uniform Dist.")
par(mfrow=c(1,1))

hist(lm.est.eps)
```

#### Coefficients

Next, the `Coefficients` section lists our estimated linear coefficients. The first will always be our Intercept (which, essentially, is a coefficient to the number 1), followed by the coefficients of our remaining predictor variables. In this case there is only one predictor variable, which we labeled Input. Our Input coefficient, is positive, leading to a positive correlation between Input (X) and Output (Y), which is also demonstrated by our plots above.

The `Pr(>|t|)` column of the Coefficients section tells us the probability of obtaining our data if the null hypothesis were true, in this case if the coefficients were actually zero. In our case the p-value of $a$ or our Input coefficient is quite low and statistically significant. We can be less confident of our intercept, with a p-value of 0.234.

#### Model

It is worth noting that [R documentation](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sigma.html) states that the term "residual standard error" that we see in our `summary` output is a misnomer, and the proper term is residual standard deviation.

Our summary output includes this calculation of the residual standard deviation, or $\sigma$ as described below, as well as R-squared (also notated as $r^{2}$ or $\rho^{2}$) which is our "coefficient of determination" or "squared correlation coefficient". Whereas the correlation coefficient, "r" measures the strength and the direction of a linear relationship between two variables, the square of the correlation coefficient, "r squared", which we see in our summary, measures the proportion of our data that is explained by the linear relationship. Thus r-squared can be considered one estimate of the "predictive power" of our model. 

In this case roughly 83% of the variation in our Output is explained by our linear model of:

$Output = `r round(lm.est.a, 3)` \cdot Input + `r round(lm.est.b, 3)`$

The p-value of the model itself is found in the F-statistic section. Our p-value is quite low indicating statistical significance in our linear model.

Thus our model explains roughtly 83% of our variation and has a statistically significant relationship between Input and Output, however our intercept estimate is not statistically significant. In other words, we are quite confident of the slope of our line, yet we're not so sure about its intercept. 

The wide gap in p-value between slope and intercept is worth further investigation.

***What is `summary(lm.est)$sigma`?***

```{r q2.2-sigma}
names(lm.est.summary)
lm.est.summary$sigma
lm.est.summary$sigma^2
```

In this case "sigma" refers to the standard deviation of our residuals, also known as the residual standard deviation. It describes the amount that the residuals vary or spread from their mean, which is theoretically zero, and hence the amount that our output variable varies or spreads from a pure linear relationship with our input variable(s). For a simple Gaussian residual, this would be the $\sigma$ value in the below equation:

$Y = aX + b + \varepsilon$

$\varepsilon \sim Norm(0, \sigma)$

***And finally we'll test calculation of sigma, the residual standard deviation:***

Check how `sigma` is calculated in the summary object by reproducing the square of it:

1. Using `var()` 

```{r q2.2-manual-1}
sample.n <- nrow(sample.data)
lm.sigma.sq.byvar <- var(lm.estimate$residuals)

# Convert degrees of freedom
lm.sigma.sq.byvar = lm.sigma.sq.byvar * (sample.n - 1) / lm.estimate$df.residual
```

2. Using only `sum()` 

```{r q2.2-manual-2}
lm.eps.deviation <- lm.estimate$residuals - mean(lm.estimate$residuals)
lm.sigma.sq.bysum <- sum(lm.eps.deviation^2) / lm.estimate$df.residual
```

Compare the two calculations with the `summary` output:

```{r q2.2-confirm}
c("By Var()" = lm.sigma.sq.byvar, 
  "By Sum()" = lm.sigma.sq.bysum, 
  "From Model" = lm.est.summary$sigma^2)
```

## 3 Analysis of Residuals

### 3.1 Residuals of the model

Observe the residuals, plot them against the input. Also plot their probability density in comparison with the normal density.

```{r 3.1}
lm.est.eps <- lm.estimate$residuals
plot(sample.data$Input, lm.est.eps)

lm.est.eps.density <- density(lm.est.eps)
plot(lm.est.eps.density, ylim = c(0, 0.5))
lines(lm.est.eps.density$x, 
      dnorm(lm.est.eps.density$x, mean = mean(lm.est.eps), sd = sd(lm.est.eps)))
```

***What do you conclude from the analysis of residuals?***

An observation of the density of the residuals is multi-modal and seems to indicate that we may be looking at two different normal distributions, rather than one.

### 3.2 Clustering the Sample

Calculate the mean values of negative residuals and positive residuals.

```{r q3.2-1}
c(Left.Mean = mean(lm.est.eps[lm.est.eps < 0]), 
  Right.Mean = mean(lm.est.eps[lm.est.eps > 0]))
```

Separate the given sample into 2 subsamples:

* One in which the residuals are below zero

* One in which they are above zero

Create a selection variable which estimates switching between the two subsamples (a value of 1 in this variable corresponds to the positive residual case, and a value of 0 corresponds to the negative residual case).

```{r q3.2-2}
select.seq.unscrambled <- as.integer(lm.est.eps > 0)
head(select.seq.unscrambled, 30)

ChoosePos <- function(j) {
  vapply(seq_along(select.seq.unscrambled), 
         function(i) {switch(select.seq.unscrambled[i] + 1, NA, sample.data[i, j])},
         numeric(1))
}

ChooseNeg <- function(j) {
  vapply(seq_along(select.seq.unscrambled), 
         function(i) {switch(select.seq.unscrambled[i] + 1, sample.data[i, j], NA)},
         numeric(1))
}

lm.subset1 <- sapply(seq_along(sample.data), ChoosePos)
lm.subset2 <- sapply(seq_along(sample.data), ChooseNeg)

head(cbind(lm.subset1, lm.subset2), 30)

# Plot the two clusters
matplot(sample.data$Input, cbind(lm.subset1[, 2], lm.subset2[,2]), 
        type = "p", col = c("green", "blue"), pch = 19, ylab = "Separated Subsamples")

plot(select.seq.unscrambled[1:100], type = "s")
```

We've divided our data into two separate sets based on a guess (an "assumption") that: 

* Rather than looking at a *single* population whose linear correlation between predictor and response behaves according to certain linear coefficients and a Gaussian residual with certain parameters,

* We are instead looking at *two* populations each of whose linear correlation between predictor and response behaves according to *a different set* of linear coefficients and residual different parameters.

In other words, we have moved to analyzing our data set according to the following two equations:

Subsample 1 (green dots): $Y_{1} = a_{1}X_{1} + b_{1} + \varepsilon_{1}$

Subsample 1 (blue dots): $Y_{2} = a_{2}X_{2} + b_{2} + \varepsilon_{2}$

### 3.3 Confusion Matrix

The problem we have, of course, is that even if we are correct that we are indeed observing two different populations, we cannot know which of our data sets belongs to which population.

Our sequence above is the map of our "guesses". We created `select.seq.unscrambled` and assigned it the following values:

* 1 = Subsample 1

* 0 = Subsample 2

Now, this is a simulation, so let's say that hypothetically we had access to the training data set and we knew in fact far every data point whether it was in Subsample 1 or the Subsample 2. 

There is a common device used to compare our estimated sequence, `select.seq.unscrambled` and the true selection sequence, let's say this was called `select.seq.true` and was mapped as above. This device is called the confusion matrix. 

To create a confusion matrix in this case, we would call the function `confusion.Matrix` from the library `caret` as follows:

`cm <- confusionMatrix(select.seq.unscrambled, select.seq.true)$table`

Let's hard code a hypothetical output of the above function:

```{r q3.3}
cm <- as.table(matrix(c(450, 42, 50, 458), ncol = 2, byrow = T, 
               dimnames = list(c("Pred.T", "Pred.F"),
                               c("Act.T", "Act.F"))))
cm
```

The the elements $C(Predicted, Actual)$ of the resultant table are:

* Number of True Negatives: $C(0,0)$
* Number of True Positives: $C(1,1)$
* Number of False Negatives: $C(0,1)$
* Number of False Positives: $C(1,0)$

Now let's demonstrate how to calculate conditional probabilities based on our confusion matrix.

There are several characteristics of prediction quality with following definitions:

* Accuracy: $P(Prediction\ correct)$
* Sensitivity: $P(Pred=1|Act=1)$
* Specificity: $P(Pred=0|Act=0)$
* Balanced Accuracy: $\frac{Sensitivity + Specificity}{2}$

Calculate accuracy, sensitivity, specificity and balanced accuracy for the confusion table above.

```{r q3.3-prob}
cm.accuracy <- (cm["Pred.T", "Act.T"] + cm["Pred.F", "Act.F"]) / 1000
cm.sensitivity <- cm["Pred.T", "Act.T"] / sum(cm[,"Act.T"])
cm.specificity <- cm["Pred.F", "Act.F"] / sum(cm[, "Act.F"])
cm.balanced <- (cm.sensitivity + cm.specificity) / 2
(cm.out <- c(Accuracy = cm.accuracy,
             Sensitivity = cm.sensitivity, 
             Specificity = cm.specificity,
             Balanced = cm.balanced))
```

## 4 Estimating models for subsamples

### 4.1 Fitting models

Now estimate the linear models from the subsamples.

```{r q4.1}
lm.subset1.estimate <- lm(Output ~ Input, 
                          data = data.frame(Input = lm.subset1[,1], Output = lm.subset1[,2]))
lm.subset2.estimate <- lm(Output ~ Input, 
                          data = data.frame(Input = lm.subset2[,1], Output = lm.subset2[,2]))
lm.subset1.est.summary <- summary(lm.subset1.estimate)
lm.subset2.est.summary <- summary(lm.subset2.estimate)

lm.subset1.est.summary$coefficients
lm.subset1.est.summary$sigma
lm.subset1.est.summary$df
lm.subset1.est.summary$r.squared
lm.subset1.est.summary$adj.r.squared

lm.subset2.est.summary$coefficients
lm.subset2.est.summary$sigma
lm.subset2.est.summary$df
lm.subset2.est.summary$r.squared
lm.subset2.est.summary$adj.r.squared
```

And write a summary comparing the fit of the subsamples to the whole sample:

* For the sigma parameters:

```{r q4.1-sigma}
c(lm.est.summary$sigma,
  lm.subset1.est.summary$sigma,
  lm.subset2.est.summary$sigma)
```

* For the $\rho^2$:

```{r q4.1-rho.sq}
c(lm.est.summary$r.squared,
  lm.subset1.est.summary$r.squared,
  lm.subset2.est.summary$r.squared)
```

* For the F-statistics:

```{r q4.1-f}
rbind(LinearModel=lm.est.summary$fstatistic,
      LinearModelPositive=lm.subset1.est.summary$fstatistic,
      LinearModelNegative=lm.subset2.est.summary$fstatistic)
```

Here is how we can calculate p-values of F-test using cumulative probability function of F-distribution:

```{r q4.1-f-manual}
lm.est.pf <- pf(lm.est.summary$fstatistic[1], 
                lm.est.summary$fstatistic[2], 
                lm.est.summary$fstatistic[3],lower.tail = FALSE)
lm.subset1.est.pf <- pf(lm.subset1.est.summary$fstatistic[1], 
                        lm.subset1.est.summary$fstatistic[2], 
                        lm.subset1.est.summary$fstatistic[3],lower.tail = FALSE)
lm.subset2.est.pf <- pf(lm.subset2.est.summary$fstatistic[1], 
                        lm.subset2.est.summary$fstatistic[2], 
                        lm.subset2.est.summary$fstatistic[3],lower.tail = FALSE)
c(LinearModel = lm.est.pf,
  LinearModelPositive = lm.subset1.est.pf,
  LinearModelNegative = lm.subset2.est.pf)
```

The numbers may not look exactly the same as in `summary()` because of the precision limitation.

Compare the combined residuals of the two separated models with the residuals of our original estimated model.

```{r q4.1-plot}
# Plot residuals
matplot(cbind(MixedModelResiduals=c(lm.subset1.est.summary$residuals,
                                    lm.subset2.est.summary$residuals),
              SingleModelResiduals=lm.est.summary$residuals),
        type="p", pch=16, ylab="Residuals before and after unscrambling")

# Estimate standard deviations
apply(cbind(MixedModelResiduals=c(lm.subset1.est.summary$residuals,
                                   lm.subset2.est.summary$residuals),
            SingleModelResiduals=lm.est.summary$residuals),2,sd)
```

And let's do a final output table to make the data easier to compare:

```{r q4.1-table}
t1.2 <- round(lm.est.summary$sigma, 4)
t1.3 <- round(lm.subset1.est.summary$sigma, 4)
t1.4 <- round(lm.subset2.est.summary$sigma, 4)
t2.2 <- round(lm.est.summary$r.squared, 4)
t2.3 <- round(lm.subset1.est.summary$r.squared, 4)
t2.4 <- round(lm.subset2.est.summary$r.squared, 4)
t3.2 <- round(lm.est.summary$coefficients[1,4], 4)
t3.3 <- round(lm.subset1.est.summary$coefficients[1,4], 4)
t3.4 <- round(lm.subset2.est.summary$coefficients[1,4], 4)
t4.2 <- round(lm.est.summary$coefficients[2,4], 4)
t4.3 <- round(lm.subset1.est.summary$coefficients[2,4], 4)
t4.4 <- round(lm.subset2.est.summary$coefficients[2,4], 4)
t5.2 <- round(lm.est.summary$fstatistic[1])
t5.3 <- round(lm.subset1.est.summary$fstatistic[1])
t5.4 <- round(lm.subset2.est.summary$fstatistic[1])
```


Estimate | Linear Model | Linear Model Positive | Linear Model Negative
---------|--------------|-----------------------|----------------------
$\sigma_{\varepsilon}$ | `r t1.2` | `r t1.3` | `r t1.4`
$\rho^{2}$ | `r t2.2` | `r t2.3` | `r t2.4`
`r dimnames(lm.est.summary$coefficients)[[1]][1]` p-value | `r t3.2` | `r t3.3` | `r t3.4`
`r dimnames(lm.est.summary$coefficients)[[1]][2]` p-value | `r t4.2` | `r t4.3` | `r t4.4`
F-statistic | `r t5.2` | `r t5.3` | `r t5.4`
F p-value | `r round(lm.est.pf, 4)` | `r round(lm.subset1.est.pf, 4)` | `r round(lm.subset2.est.pf, 4)`

***What is the difference between the quality of fit?***

??? XXX ???

***What is the difference between the two estimated models?***

??? XXX ???

***Try to guess how the model data were simulated and with what parameters?***

??? XXX ???

## Test

```{r test}
dat <- read.table('Week4_Test_Sample.csv', header=TRUE)

test.fit <- lm(Y~X, dat)
test.fit.summary <- summary(test.fit)
test.fit.summary

plot(dat$X, dat$Y)
y.est <- dat$X * coef(test.fit)[2] + coef(test.fit)[1]
plot(dat$X, y.est)

test.eps <- dat$Y - y.est
par(mfrow=c(1,2))
plot(test.eps)
boxplot(test.eps)
hist(test.eps)
eps.density <- density(test.eps)
plot(eps.density)
par(mfrow=c(1,1))

# Cluster
c(Left.Mean = mean(test.eps[test.eps < 0]), 
  Right.Mean = mean(test.eps[test.eps > 0]))
select.seq.unscrambled <- as.integer(test.eps > 0)
head(select.seq.unscrambled, 30)

ChoosePos <- function(j) {
  vapply(seq_along(select.seq.unscrambled), 
         function(i) {switch(select.seq.unscrambled[i] + 1, NA, dat[i, j])},
         numeric(1))
}

ChooseNeg <- function(j) {
  vapply(seq_along(select.seq.unscrambled), 
         function(i) {switch(select.seq.unscrambled[i] + 1, dat[i, j], NA)},
         numeric(1))
}

lm.subset1 <- sapply(seq_along(dat), ChoosePos)
lm.subset2 <- sapply(seq_along(dat), ChooseNeg)

head(cbind(lm.subset1, lm.subset2), 30)

# Plot the two clusters
matplot(dat$X, cbind(lm.subset1[, 2], lm.subset2[, 2]), 
        type = "p", col = c("green", "blue"), pch = 19, ylab = "Separated Subsamples")

plot(select.seq.unscrambled[1:100], type = "s")

res <- list(Unscrambled.Selection.Sequence =  select.seq.unscrambled)
write.table(res, file = 'result.csv', row.names = F)
```

